# S3 연결
spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.aws.credentials.provider=com.amazonaws.auth.DefaultAWSCredentialsProviderChain
spark.hadoop.fs.s3a.fast.upload=true
spark.hadoop.fs.s3a.multipart.size=134217728
spark.hadoop.fs.s3a.fast.upload.active.blocks=4

# S3A committers 활성화
spark.hadoop.mapreduce.outputcommitter.factory.class=org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory
spark.hadoop.fs.s3a.committer.name=directory
spark.hadoop.fs.s3a.committer.magic.enabled=true
spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2
spark.hadoop.fs.s3a.committer.staging.conflict-mode=replace
spark.hadoop.fs.s3a.committer.staging.unique-filenames=true
spark.hadoop.fs.s3a.fast.upload=true

# Committer 설정
spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2
spark.sql.sources.commitProtocolClass=org.apache.spark.internal.io.cloud.PathOutputCommitProtocol
spark.sql.parquet.output.committer.class=org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter

# 이벤트 로그 (팀/개인 분기)
# 팀
spark.eventLog.enabled=true
spark.eventLog.dir=s3a://softeer-de-6th-team1/spark-events
spark.history.fs.logDirectory=s3a://softeer-de-6th-team1/spark-events

# 개인
# spark.eventLog.enabled=true
# spark.eventLog.dir=/home/softeer/spark-events
# spark.history.fs.logDirectory=/home/softeer/spark-events
