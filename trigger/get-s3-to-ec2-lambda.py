import json
import boto3
import os

# ## ì„¤ì •ì´ í•„ìš”í•œ ë³€ìˆ˜ ##
# Spark ì‘ì—…ì„ ì‹¤í–‰í•  EC2 ì¸ìŠ¤í„´ìŠ¤ì˜ IDë¥¼ ì…ë ¥í•˜ì„¸ìš”.
TARGET_INSTANCE_ID = "i-083205ced665c9918" 
# spark-submit ìŠ¤í¬ë¦½íŠ¸ê°€ ìˆëŠ” ì»¨í…Œì´ë„ˆ ë‚´ ê²½ë¡œ
SPARK_JOB_SCRIPT = "/home/softeer/jobs/spark-job.py" 
# spark-master ì»¨í…Œì´ë„ˆ ì´ë¦„
CONTAINER_NAME = "spark-master"

ssm_client = boto3.client('ssm')

def lambda_handler(event, context):
    # 1. S3 ì´ë²¤íŠ¸ì—ì„œ ë²„í‚· ì´ë¦„ê³¼ íŒŒì¼ ê²½ë¡œ(key) ì¶”ì¶œ
    s3_event = event['Records'][0]['s3']
    bucket_name = s3_event['bucket']['name']
    trigger_file_key = s3_event['object']['key'] # ì˜ˆ: "raw-data/20250825T110000Z/_SUCCESS"

    # S3 ì „ì²´ ê²½ë¡œ 
    folder_key = os.path.dirname(trigger_file_key) # ê²°ê³¼: "raw-data/20250825T110000Z"    
    s3_folder_path = f"s3a://{bucket_name}/{folder_key}/"
    print(f"Trigger file detected: s3a://{bucket_name}/{trigger_file_key}")
    print(f"ğŸ¯ Target data folder for Spark: {s3_folder_path}")

    # 2. EC2 ì»¨í…Œì´ë„ˆì—ì„œ ì‹¤í–‰í•  spark-submit ëª…ë ¹ì–´ ìƒì„±
    spark_command = (
        f"/opt/spark/bin/spark-submit --master spark://spark-master:7077 --deploy-mode client {SPARK_JOB_SCRIPT} {s3_folder_path}"
    )

    # bash -cë¥¼ ì´ìš©í•´ ì „ì²´ ëª…ë ¹ì„ ê°ì‹¸ê³ , ì¶œë ¥ì„ íŒŒì¼ë¡œ ë¦¬ë””ë ‰ì…˜í•©ë‹ˆë‹¤.
    command_to_run = (
        f"docker exec {CONTAINER_NAME} "
        f"bash -c 'unset AWS_PROFILE && {spark_command} > /home/softeer/jobs/spark-job.log 2>&1'"
    )

    print(f"ì‹¤í–‰í•  ëª…ë ¹ì–´: {command_to_run}")

    try:
        # 3. SSM Run Commandë¥¼ í†µí•´ EC2ì— ëª…ë ¹ì–´ ì „ì†¡
        response = ssm_client.send_command(
            InstanceIds=[TARGET_INSTANCE_ID],
            DocumentName="AWS-RunShellScript",
            Parameters={'commands': [command_to_run]},
            Comment=f"Trigger Spark job for {folder_key}"
        )
        command_id = response['Command']['CommandId']
        print(f"SSM Command ID: {command_id} - Command sent successfully")

    except Exception as e:
        print(f"Error occurred: {e}")
        raise e

    return {
        'statusCode': 200,
        'body': json.dumps(f'Spark job triggered for {s3_folder_path}')
    }